{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()\n",
    "\n",
    "multiSentence = \"There is an art, it says, or rather, a knack to flying.\" \\\n",
    "                 \"The knack lies in learning how to throw yourself at the ground and miss.\" \\\n",
    "                 \"In the beginning the Universe was created. This has made a lot of people \"\\\n",
    "                 \"very angry and been widely regarded as a bad move.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TOKENS,POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "There is an art, it says, or rather, a knack to flying.The knack lies in learning how to throw yourself at the ground and miss.In the beginning the Universe was created. This has made a lot of people very angry and been widely regarded as a bad move."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData = parser(multiSentence)\n",
    "parsedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 769 There\n",
      "lowercased: 608 there\n",
      "lemma: 608 there\n",
      "shape: 684 Xxxxx\n",
      "prefix: 568 T\n",
      "suffix: 609 ere\n",
      "log probability: -7.277902603149414\n",
      "Brown cluster id: 1918\n",
      "----------------------------------------\n",
      "original: 513 is\n",
      "lowercased: 513 is\n",
      "lemma: 536 be\n",
      "shape: 505 xx\n",
      "prefix: 509 i\n",
      "suffix: 513 is\n",
      "log probability: -4.3297648429870605\n",
      "Brown cluster id: 762\n",
      "----------------------------------------\n",
      "original: 591 an\n",
      "lowercased: 591 an\n",
      "lemma: 591 an\n",
      "shape: 505 xx\n",
      "prefix: 506 a\n",
      "suffix: 591 an\n",
      "log probability: -5.953293800354004\n",
      "Brown cluster id: 3\n",
      "----------------------------------------\n",
      "original: 879 art\n",
      "lowercased: 879 art\n",
      "lemma: 879 art\n",
      "shape: 502 xxx\n",
      "prefix: 506 a\n",
      "suffix: 879 art\n",
      "log probability: -9.778430938720703\n",
      "Brown cluster id: 633\n",
      "----------------------------------------\n",
      "original: 450 ,\n",
      "lowercased: 450 ,\n",
      "lemma: 450 ,\n",
      "shape: 450 ,\n",
      "prefix: 450 ,\n",
      "suffix: 450 ,\n",
      "log probability: -3.3914804458618164\n",
      "Brown cluster id: 4\n",
      "----------------------------------------\n",
      "original: 519 it\n",
      "lowercased: 519 it\n",
      "lemma: 757862 -PRON-\n",
      "shape: 505 xx\n",
      "prefix: 509 i\n",
      "suffix: 519 it\n",
      "log probability: -4.5064496994018555\n",
      "Brown cluster id: 474\n",
      "----------------------------------------\n",
      "original: 1053 says\n",
      "lowercased: 1053 says\n",
      "lemma: 692 say\n",
      "shape: 515 xxxx\n",
      "prefix: 600 s\n",
      "suffix: 842 ays\n",
      "log probability: -8.204558372497559\n",
      "Brown cluster id: 244\n",
      "----------------------------------------\n",
      "original: 450 ,\n",
      "lowercased: 450 ,\n",
      "lemma: 450 ,\n",
      "shape: 450 ,\n",
      "prefix: 450 ,\n",
      "suffix: 450 ,\n",
      "log probability: -3.3914804458618164\n",
      "Brown cluster id: 4\n",
      "----------------------------------------\n",
      "original: 563 or\n",
      "lowercased: 563 or\n",
      "lemma: 563 or\n",
      "shape: 505 xx\n",
      "prefix: 511 o\n",
      "suffix: 563 or\n",
      "log probability: -5.715355396270752\n",
      "Brown cluster id: 404\n",
      "----------------------------------------\n",
      "original: 1017 rather\n",
      "lowercased: 1017 rather\n",
      "lemma: 1017 rather\n",
      "shape: 515 xxxx\n",
      "prefix: 659 r\n",
      "suffix: 656 her\n",
      "log probability: -8.124470710754395\n",
      "Brown cluster id: 6698\n",
      "----------------------------------------\n",
      "original: 450 ,\n",
      "lowercased: 450 ,\n",
      "lemma: 450 ,\n",
      "shape: 450 ,\n",
      "prefix: 450 ,\n",
      "suffix: 450 ,\n",
      "log probability: -3.3914804458618164\n",
      "Brown cluster id: 4\n",
      "----------------------------------------\n",
      "original: 506 a\n",
      "lowercased: 506 a\n",
      "lemma: 506 a\n",
      "shape: 507 x\n",
      "prefix: 506 a\n",
      "suffix: 506 a\n",
      "log probability: -3.9830753803253174\n",
      "Brown cluster id: 19\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, token in enumerate(parsedData):\n",
    "    print(\"original:\", token.orth, token.orth_)\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    print(\"----------------------------------------\")\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an art, it says, or rather, a knack to flying.\n",
      "The knack lies in learning how to throw yourself at the ground and miss.\n",
      "In the beginning the Universe was created.\n",
      "This has made a lot of people very angry and been widely regarded as a bad move.\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "\n",
    "for span in parsedData.sents:\n",
    "    \n",
    "    sent = ''.join(parsedData[i].string for i in range(span.start, span.end)).strip()\n",
    "    sents.append(sent)\n",
    "\n",
    "for sentence in sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There ADV\n",
      "is VERB\n",
      "an DET\n",
      "art NOUN\n",
      ", PUNCT\n",
      "it PRON\n",
      "says VERB\n",
      ", PUNCT\n",
      "or CCONJ\n",
      "rather ADV\n",
      ", PUNCT\n",
      "a DET\n",
      "knack NOUN\n",
      "to ADP\n",
      "flying NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for span in parsedData.sents:\n",
    "    sent = [parsedData[i] for i in range(span.start, span.end)]\n",
    "    break\n",
    "\n",
    "for token in sent:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det boy [] []\n",
      "boy nsubj ran ['The'] ['with']\n",
      "with prep boy [] []\n",
      "the det dog [] []\n",
      "spotted amod dog [] []\n",
      "dog nsubj ran ['the', 'spotted'] []\n",
      "quickly advmod ran [] []\n",
      "ran ROOT ran ['boy', 'dog', 'quickly'] ['after', '.']\n",
      "after prep ran [] ['firetruck']\n",
      "the det firetruck [] []\n",
      "firetruck pobj after ['the'] []\n",
      ". punct ran [] []\n"
     ]
    }
   ],
   "source": [
    "example = \"The boy with the spotted dog quickly ran after the firetruck.\"\n",
    "parsedEx = parser(example)\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "'s (not an entity)\n",
      "stocks (not an entity)\n",
      "dropped (not an entity)\n",
      "dramatically (not an entity)\n",
      "after (not an entity)\n",
      "the (not an entity)\n",
      "death (not an entity)\n",
      "of (not an entity)\n",
      "Steve PERSON\n",
      "Jobs PERSON\n",
      "in (not an entity)\n",
      "October DATE\n",
      ". (not an entity)\n",
      "380 ORG Apple\n",
      "377 PERSON Steve Jobs\n",
      "387 DATE October\n"
     ]
    }
   ],
   "source": [
    "example = \"Apple's stocks dropped dramatically after the death of Steve Jobs in October.\"\n",
    "parsedEx = parser(example)\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.ent_type_ if token.ent_type_ != \"\" else \"(not an entity)\")\n",
    "\n",
    "ents = list(parsedEx.ents)\n",
    "for entity in ents:\n",
    "    print(entity.label, entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol NOUN lol\n",
      "that ADJ that\n",
      "is VERB be\n",
      "rly ADV rly\n",
      "funny ADJ funny\n",
      ":) PUNCT :)\n",
      "This DET this\n",
      "is VERB be\n",
      "gr8 VERB gr8\n",
      "i PRON i\n",
      "rate VERB rate\n",
      "it PRON -PRON-\n",
      "8/8 NUM 8/8\n",
      "! PUNCT !\n",
      "! PUNCT !\n",
      "! PUNCT !\n"
     ]
    }
   ],
   "source": [
    "messyData = \"lol that is rly funny :) This is gr8 i rate it 8/8!!!\"\n",
    "parsedData = parser(messyData)\n",
    "for token in parsedData:\n",
    "    print(token.orth_, token.pos_, token.lemma_)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-c:6: RuntimeWarning: invalid value encountered in float_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most similar words to NASA:\n",
      "consistency\n",
      "hotter\n",
      "journey\n",
      "featured\n",
      "ref\n",
      "puppies\n",
      "artwork\n",
      "crystal\n",
      "defenses\n",
      "helmet\n",
      "slowed\n",
      "promotion\n",
      "similarities\n",
      "boner\n",
      "flex\n",
      "flew\n",
      "adapter\n",
      "acquire\n",
      "quarters\n",
      "sizes\n",
      "\n",
      "----------------------------\n",
      "Top 3 closest results for king - man + woman:\n",
      "queen\n",
      "kings\n",
      "princess\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "nasa = parser.vocab['NASA']\n",
    "\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"nasa\"})\n",
    "\n",
    "allWords.sort(key=lambda w: cosine(w.vector, nasa. vector))\n",
    "allWords.reverse()\n",
    "print(\"Top 20 most similar words to NASA:\")\n",
    "for word in allWords[:20]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "\n",
    "king = parser.vocab['king']\n",
    "man = parser.vocab['man']\n",
    "woman = parser.vocab['woman']\n",
    "\n",
    "result = king.vector - man.vector + woman.vector\n",
    "\n",
    "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"king\" and w.lower_ != \"man\" and w.lower_ != \"woman\"})\n",
    "allWords.sort(key=lambda w: cosine(w.vector, result))\n",
    "allWords.reverse()\n",
    "print(\"\\n----------------------------\\nTop 3 closest results for king - man + woman:\")\n",
    "for word in allWords[:3]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "# it got it! Queen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('he', 'shot', 'me'), ('he', 'shot', 'sister'), ('brother', 'shot', 'me'), ('brother', 'shot', 'sister')]\n",
      "[('watches', 'are', 'idea'), ('which', 'was', 'this'), ('it', 'was', 'pieces')]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# machine learning example with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "results:\n",
      "i h8 riting comprehensibly #skoolsux : twitter\n",
      "planets and stars and rockets and stuff : space\n",
      "accuracy: 1.0\n",
      "----------------------------------------------------------------------------------------------\n",
      "Top 10 features used to predict: \n",
      "Class 1 best: \n",
      "(-0.5317457349252519, 'planet')\n",
      "(-0.35387821917377232, 'space')\n",
      "(-0.21950314690393238, 'mar')\n",
      "(-0.21950314690393238, 'red')\n",
      "(-0.15678762293683152, 'earth')\n",
      "(-0.15678762293683152, 'launch')\n",
      "(-0.15678762293683152, 'rocket')\n",
      "(-0.14909864994270897, 'great')\n",
      "(-0.14909864994270897, 'love')\n",
      "(-0.099774045796133873, 'blue')\n",
      "Class 2 best: \n",
      "(0.40866490475230544, 'twitter')\n",
      "(0.35268312478964314, '@mention')\n",
      "(0.22672529221454771, 'lol')\n",
      "(0.22672529221454771, 'gr8')\n",
      "(0.20433286130934575, 'social')\n",
      "(0.20433286130934575, 'medium')\n",
      "(0.20433204344295969, 'reddit')\n",
      "(0.20433204344295969, 'fun')\n",
      "(0.12595783257509538, 'window')\n",
      "(0.12595783257509538, 'u')\n",
      "----------------------------------------------------------------------------------------------\n",
      "The original data as it appeared to the classifier after tokenizing, lemmatizing, stoplisting, etc\n",
      "Sample 0: ('great', 1)('space', 2)('love', 1)\n",
      "Sample 1: ('exist', 1)('glad', 1)('cool', 1)('planet', 1)('space', 1)\n",
      "Sample 2: ('gr8', 1)('@mention', 1)('lol', 1)\n",
      "Sample 3: ('fun', 1)('reddit', 1)('twitter', 1)\n",
      "Sample 4: ('red', 1)('mar', 1)('planet', 1)\n",
      "Sample 5: ('9', 1)('window', 1)('skip', 1)('u', 1)('@mention', 1)\n",
      "Sample 6: ('earth', 1)('launch', 1)('rocket', 1)('planet', 1)\n",
      "Sample 7: ('medium', 1)('social', 1)('twitter', 1)\n",
      "Sample 8: ('hashtag', 1)('@mention', 3)\n",
      "Sample 9: ('green', 1)('blue', 1)('little', 1)('sun', 1)('orbit', 1)('planet', 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\"]\n",
    "\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "  \n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "def cleanText(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    \n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# def printNMostInformative(vectorizer, clf, N):\n",
    "#     feature_names = vectorizer.get_feature_names()\n",
    "#     coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "#     topClass1 = coefs_with_fns[:N]\n",
    "#     topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "#     print(\"Class 1 best: \")\n",
    "#     for feat in topClass1:\n",
    "#         print(feat)\n",
    "#     print(\"Class 2 best: \")\n",
    "#     for feat in topClass2:\n",
    "#         print(feat)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
    "clf = LinearSVC()\n",
    "model = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "\n",
    "train = [\"I love space. Space is great.\", \"Planets are cool. I am glad they exist in space\", \"lol @twitterdude that is gr8\", \n",
    "        \"twitter &amp; reddit are fun.\", \"Mars is a planet. It is red.\", \"@Microsoft: y u skip windows 9?\", \"Rockets launch from Earth and go to other planets.\",\n",
    "        \"twitter social media &gt; &lt;\", \"@someguy @somegirl @twitter #hashtag\", \"Orbiting the sun is a little blue-green planet.\"]\n",
    "labelsTrain = [\"space\", \"space\", \"twitter\", \"twitter\", \"space\", \"twitter\", \"space\", \"twitter\", \"twitter\", \"space\"]\n",
    "\n",
    "test = [\"i h8 riting comprehensibly #skoolsux\", \"planets and stars and rockets and stuff\"]\n",
    "labelsTest = [\"twitter\", \"space\"]\n",
    "\n",
    "model.fit(train, labelsTrain)\n",
    "\n",
    "preds = model.predict(test)\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"results:\")\n",
    "for (sample, pred) in zip(test, preds):\n",
    "    print(sample, \":\", pred)\n",
    "print(\"accuracy:\", accuracy_score(labelsTest, preds))\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"Top 10 features used to predict: \")\n",
    "# show the top features\n",
    "printNMostInformative(vectorizer, clf, 10)\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"The original data as it appeared to the classifier after tokenizing, lemmatizing, stoplisting, etc\")\n",
    "# let's see what the pipeline was transforming the data into\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n",
    "transform = pipe.fit_transform(train, labelsTrain)\n",
    "\n",
    "# get the features that the vectorizer learned (its vocabulary)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# the values from the vectorizer transformed data (each item is a row,column index with value as # times occuring in the sample, stored as a sparse matrix)\n",
    "for i in range(len(train)):\n",
    "    s = \"\"\n",
    "    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    for idx, num in zip(indexIntoVocab, numOccurences):\n",
    "        s += str((vocab[idx], num))\n",
    "    print(\"Sample {}: {}\".format(i, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
